{"cells":[{"cell_type":"markdown","source":["You should remove the `raise` exceptions below and insert your code in their place. The cells which say `DO NOT CHANGE THE CONTENT OF THIS CELL` are there to help you, if they fail, it's probably an indication of the fact that your code is wrong. You should not change their content - if you change them to make them correspond to what your program is producing, you will still not get the marks.\n\nIf you encounter an error while running your notebook that doesn't appear to be connected to RDDs (such as missing `imp`), you should check that you've run the initialization cells since you've started your latest cluster.\n\nBefore you turn your solution in, make sure everything runs as expected. With an attached cluster, you should **Clear State and Results** (under the **Clear** dropdown menu) and then click on the **Run all** icon. This runs all cells in the notebook from new. You should only submit this notebook if all cells run.\n\nThis homework is to be completed on your own. By the act of following these instructions and handing your work in, it is deemed that you have read and understand the rules on plagiarism as written in your student handbook."],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"advice","locked":true,"solution":false,"checksum":"0d6114a510fa3eff84cc1bb2cda180e2","grade":false,"cell_type":"markdown"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6b4476b-b847-4602-b83a-747e341cc171"}}},{"cell_type":"markdown","source":["We will use the historical World cup player dataset (the second cell of this notebook downloads this for you) which is in JSON format. The first two cells set the environment up for you, including downloading the file. The initial dataframe is also created for you, so your work starts when you start exploring the data in the three ways we have seen in lectures."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c710ec7-78fb-459f-9f06-9a4b4901bac8"}}},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\nimport sys\nif not 'dbruntime.dbutils' in sys.modules.keys():\n    import pyspark\n    sc = pyspark.SparkContext()\n    from pyspark.sql import SQLContext\n    spark = SQLContext(sc)\n    print(\"Unless you're grading this homework, you should be running this on Databricks.\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"setup-pyspark","locked":true,"solution":false,"checksum":"1129e2ad4192ea309170d290bfa0a137","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7310d6b6-a2ed-4158-a7ba-a427b871b221"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\nimport urllib.request\n\nplayer_json = \"/FileStore/all-world-cup-players.json\"\nif 'dbruntime.dbutils' in sys.modules.keys():\n    try:\n        dbutils.fs.ls(file1)\n    except:\n        # Download to local /tmp\n        urllib.request.urlretrieve(\"https://github.com/jokecamp/FootballData/raw/master/World%20Cups/all-world-cup-players.json\", \"/tmp/all-world-cup-players.json\")\n        # Copy to DBFS\n        dbutils.fs.cp(\"file:/tmp/all-world-cup-players.json\", player_json)"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"setup-file","locked":true,"solution":false,"checksum":"b5cb840dc782c6c747a843fa256b4cfd","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"194e8a9b-c792-4c3f-9753-078b1d9ac311"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The cell below reads the data into a dataframe named `playersDF`. If you look at the file, you'll see that it's not formed quite as Spark expects: it doesn't have a single line per json record. We therefore use the `multiline` option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab2f00ac-022e-4828-afc4-3e43027a5496"}}},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\nplayersDF = spark.read.option(\"multiline\",\"true\").json(player_json)\nassert playersDF.count() == 9443, \"Something has gone wrong with the reading process\""],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"setup-df","locked":true,"solution":false,"checksum":"615c5556fd6aeab04e835fafaf8b7d27","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f202a7ca-9ceb-4f3c-9d92-08c05964ee51"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We will now explore three different ways to extract the same information from the data. \n1. Via DataFrames directly\n2. Via Views\n3. Via RDDs\n\nLet's start with the DataFrames. Use DataFrame operations to create a `teamNamesFromDF` DataFrame which contains all the team names from 2014 (only). (You may want to look at the DataFrame you have read in first.) The team names should only appear once in your resulting dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4ff4d09-d869-47a8-9ec9-2cbb96063f99"}}},{"cell_type":"code","source":["# Your answer should have the format\n# teamNamesFromDF = ...\n# YOUR CODE HERE\nteamNamesFromDF = playersDF.select(\"Team\").where(\"Year==2014\").dropDuplicates()\nteamNamesFromDF.count()"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"teamNamesFromDF","locked":false,"solution":true,"checksum":"e770b3adabadec6fdbfdfa8f1bb39e5d","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cd1f395-fd4f-4748-abbc-9ca09bc697cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: 32","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: 32"]}}],"execution_count":0},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\nfrom pyspark.sql import DataFrame\nassert isinstance(teamNamesFromDF, DataFrame), \"Your answer should be a dataframe\"\nassert teamNamesFromDF.count() == 32, \"Unexpected number of teams\""],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_teamNamesFromDF","locked":true,"solution":false,"points":2,"checksum":"72ae43c72c61b37780a8c39691e7d703","grade":true,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93ca1484-27d6-4e87-8b0b-da68c59325dd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now do the same via constructing a temporary view called `players` from the data (remember that you'll need to ensure that the program doesn't fail if you run it twice - i.e. if the view already exists), using a Spark sql query to extract the 2014 team names (without repeats), and naming the resulting DataFrame `teamNamesFromTable`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6692ba9c-1df3-470b-8481-5a5f3f4a69c7"}}},{"cell_type":"code","source":["# Your answer should have the format\n# teamNamesFromTable =\n# YOUR CODE HERE\nplayersDF.createOrReplaceTempView(\"players\")\nteamNamesFromTable = spark.sql(\"SELECT team FROM players WHERE year==2014\").dropDuplicates()\nteamNamesFromTable.count()\nteamNamesFromTable.show()"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"teamNamesFromTable","locked":false,"solution":true,"checksum":"bbe2bb8d451ac00eb0b73f14811df91e","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32c66195-4040-46f3-99bf-d278b3d087a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------+\n|       team|\n+-----------+\n|Ivory Coast|\n|     France|\n|     Greece|\n|  Argentina|\n|    Ecuador|\n|      Chile|\n|    Croatia|\n|      Italy|\n|      Spain|\n|    Uruguay|\n|     Mexico|\n|   Honduras|\n|Switzerland|\n|     Brazil|\n|      Japan|\n|    England|\n|   Cameroon|\n|  Australia|\n| Costa Rica|\n|   Colombia|\n+-----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+\n|       team|\n+-----------+\n|Ivory Coast|\n|     France|\n|     Greece|\n|  Argentina|\n|    Ecuador|\n|      Chile|\n|    Croatia|\n|      Italy|\n|      Spain|\n|    Uruguay|\n|     Mexico|\n|   Honduras|\n|Switzerland|\n|     Brazil|\n|      Japan|\n|    England|\n|   Cameroon|\n|  Australia|\n| Costa Rica|\n|   Colombia|\n+-----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\n# Check the table was created\nif 'dbruntime.dbutils' in sys.modules.keys():\n    tableList = [t.name for t in spark.catalog.listTables()]\n    assert \"players\" in tableList, \"You have either not created your table or you have named it something other than players\"\nelse:\n    assert ('players' in spark.tableNames()), \"You have either not created your table or you have named it something other than players\"\n    \nassert teamNamesFromTable.count() == 32, \"Unexpected number of teams\""],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_teamNamesFromTable","locked":true,"solution":false,"points":2,"checksum":"4d953c4882b8f90c7f3507c7fc2f9eaa","grade":true,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"909d74bd-e1f8-4a2c-928e-885f5dfd01a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Your third implementation should go via RDDs: i.e. you'll need to create a (Row) RDD from the DataFrame data, perform `.map`, `.filter` etc operations to obtain an RDD with the same result using RDDs. Your resulting RDD should be named `teamNamesFromRDD`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"358e6c3d-f56a-4323-bdff-82ddb731cdbc"}}},{"cell_type":"code","source":["# Your answer should have the format\n# teamNamesFromRDD = ...\n# YOUR CODE HERE\nROWplayersrdd= playersDF.rdd\nteamNamesFromRDD = ROWplayersrdd.filter(lambda x: x[9]==2014).map(lambda line: line[8]).distinct()\nteamNamesFromRDD.count()"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"teamNamesFromRDD","locked":false,"solution":true,"checksum":"262d8c73a85627346e4a3f4ef83b0219","grade":false,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dd182ab-b79c-413c-9f0f-4e64d23fcd7c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: 32","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: 32"]}}],"execution_count":0},{"cell_type":"code","source":["# DO NOT CHANGE THE CONTENT OF THIS CELL\nfrom pyspark.rdd import RDD\nassert isinstance(teamNamesFromRDD, RDD), \"Your result should be an RDD\"\nassert teamNamesFromRDD.count() == 32, \"Unexpected number of teams\"\n\nlineage = teamNamesFromRDD.toDebugString()\nassert 'MapPartitionsRDD' in lineage.decode(), \"Did you really manage to answer this question via RDDs without a map?\""],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_teamNamesFromRDD","locked":true,"solution":false,"points":2,"checksum":"9da6655e818c829b41c49690dd7c3dff","grade":true,"cell_type":"code"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e949162b-af6b-42e1-ae31-255656d2fb5d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a2479fa-b58a-4f4e-953d-09fa0447b88b"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.7","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"spark_sql week5","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1107644834885029}},"nbformat":4,"nbformat_minor":0}
